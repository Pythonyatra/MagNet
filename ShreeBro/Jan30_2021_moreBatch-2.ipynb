{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fjf8_gP6Whhk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OzoDj0MUDDGv",
    "outputId": "da016d07-e6f0-4a15-a8fe-ce871c61b646"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_jIxtPcDesa",
    "outputId": "7d94a148-6b3c-414f-ef32-63f6cf92c9ae"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '/Users/shovitraj/Github/Python/MagNet_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OEpID3GJDDGx"
   },
   "outputs": [],
   "source": [
    "dst = pd.read_csv(DATA_PATH +\"dst_labels.csv\")\n",
    "dst.timedelta = pd.to_timedelta(dst.timedelta)\n",
    "dst.set_index([\"period\", \"timedelta\"], inplace=True)\n",
    "\n",
    "sunspots = pd.read_csv(DATA_PATH + \"sunspots.csv\")\n",
    "sunspots.timedelta = pd.to_timedelta(sunspots.timedelta)\n",
    "sunspots.set_index([\"period\", \"timedelta\"], inplace=True)\n",
    "\n",
    "solar_wind = pd.read_csv(DATA_PATH + \"solar_wind.csv\")\n",
    "solar_wind.timedelta = pd.to_timedelta(solar_wind.timedelta)\n",
    "solar_wind.set_index([\"period\", \"timedelta\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UPq1cKgADDGx"
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "G34dgNLZDDGy"
   },
   "outputs": [],
   "source": [
    "seed(2020)\n",
    "set_seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6DQ_qBZRDDGy"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AQM3C8kHDDGy"
   },
   "outputs": [],
   "source": [
    "# subset of solar wind features to use for modeling\n",
    "SOLAR_WIND_FEATURES = [\n",
    "    \"bx_gse\",\n",
    "    \"by_gse\",\n",
    "    \"bz_gse\",\n",
    "    \"speed\",\n",
    "    \"density\",\n",
    "    \"temperature\",\n",
    "    'theta_gse',\n",
    "    'phi_gse'\n",
    "]\n",
    "\n",
    "# all of the features we'll use, including sunspot numbers\n",
    "XCOLS = (\n",
    "    [col + \"_mean\" for col in SOLAR_WIND_FEATURES]\n",
    "    + [col + \"_std\" for col in SOLAR_WIND_FEATURES]\n",
    "    + [\"smoothed_ssn\"]\n",
    ")\n",
    "\n",
    "\n",
    "def impute_features(feature_df):\n",
    "    \"\"\"Imputes data using the following methods:\n",
    "    - `smoothed_ssn`: forward fill\n",
    "    - `solar_wind`: interpolation\n",
    "    \"\"\"\n",
    "    # forward fill sunspot data for the rest of the month\n",
    "    feature_df.smoothed_ssn = feature_df.smoothed_ssn.fillna(method=\"ffill\")\n",
    "    # interpolate between missing solar wind values\n",
    "    feature_df = feature_df.interpolate()\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "def aggregate_hourly(feature_df, aggs=[\"mean\", \"std\"]):\n",
    "    \"\"\"Aggregates features to the floor of each hour using mean and standard deviation.\n",
    "    e.g. All values from \"11:00:00\" to \"11:59:00\" will be aggregated to \"11:00:00\".\n",
    "    \"\"\"\n",
    "    # group by the floor of each hour use timedelta index\n",
    "    agged = feature_df.groupby(\n",
    "        [\"period\", feature_df.index.get_level_values(1).floor(\"H\")]\n",
    "    ).agg(aggs)\n",
    "    # flatten hierachical column index\n",
    "    agged.columns = [\"_\".join(x) for x in agged.columns]\n",
    "    return agged\n",
    "\n",
    "\n",
    "def preprocess_features(solar_wind, sunspots, scaler=None, subset=None):\n",
    "    \"\"\"\n",
    "    Preprocessing steps:\n",
    "        - Subset the data\n",
    "        - Aggregate hourly\n",
    "        - Join solar wind and sunspot data\n",
    "        - Scale using standard scaler\n",
    "        - Impute missing values\n",
    "    \"\"\"\n",
    "    # select features we want to use\n",
    "    if subset:\n",
    "        solar_wind = solar_wind[subset]\n",
    "\n",
    "    # aggregate solar wind data and join with sunspots\n",
    "    hourly_features = aggregate_hourly(solar_wind).join(sunspots)\n",
    "\n",
    "    # subtract mean and divide by standard deviation\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(hourly_features)\n",
    "\n",
    "    normalized = pd.DataFrame(\n",
    "        scaler.transform(hourly_features),\n",
    "        index=hourly_features.index,\n",
    "        columns=hourly_features.columns,\n",
    "    )\n",
    "\n",
    "    # impute missing values\n",
    "    imputed = impute_features(normalized)\n",
    "\n",
    "    # we want to return the scaler object as well to use later during prediction\n",
    "    return imputed, scaler\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "QlJa99MeW6rE",
    "outputId": "8a89adbe-4dcb-47cb-c0cc-862ef1a18a6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139872, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>bx_gse_mean</th>\n",
       "      <th>bx_gse_std</th>\n",
       "      <th>by_gse_mean</th>\n",
       "      <th>by_gse_std</th>\n",
       "      <th>bz_gse_mean</th>\n",
       "      <th>bz_gse_std</th>\n",
       "      <th>speed_mean</th>\n",
       "      <th>speed_std</th>\n",
       "      <th>density_mean</th>\n",
       "      <th>density_std</th>\n",
       "      <th>temperature_mean</th>\n",
       "      <th>temperature_std</th>\n",
       "      <th>theta_gse_mean</th>\n",
       "      <th>theta_gse_std</th>\n",
       "      <th>phi_gse_mean</th>\n",
       "      <th>phi_gse_std</th>\n",
       "      <th>smoothed_ssn</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>period</th>\n",
       "      <th>timedelta</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">train_a</th>\n",
       "      <th>0 days 00:00:00</th>\n",
       "      <td>-1.599207</td>\n",
       "      <td>-0.381502</td>\n",
       "      <td>0.419516</td>\n",
       "      <td>0.031658</td>\n",
       "      <td>0.300358</td>\n",
       "      <td>-0.651645</td>\n",
       "      <td>-0.738546</td>\n",
       "      <td>0.862524</td>\n",
       "      <td>-0.775827</td>\n",
       "      <td>-0.205724</td>\n",
       "      <td>-0.375267</td>\n",
       "      <td>0.383941</td>\n",
       "      <td>0.262090</td>\n",
       "      <td>-1.146417</td>\n",
       "      <td>-0.364011</td>\n",
       "      <td>-0.705389</td>\n",
       "      <td>0.139444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 01:00:00</th>\n",
       "      <td>-1.757995</td>\n",
       "      <td>-0.867747</td>\n",
       "      <td>0.179257</td>\n",
       "      <td>-0.272971</td>\n",
       "      <td>0.446103</td>\n",
       "      <td>-0.517913</td>\n",
       "      <td>-0.986904</td>\n",
       "      <td>0.995063</td>\n",
       "      <td>-0.861692</td>\n",
       "      <td>-0.058215</td>\n",
       "      <td>-0.479430</td>\n",
       "      <td>0.953178</td>\n",
       "      <td>0.378546</td>\n",
       "      <td>-1.054098</td>\n",
       "      <td>-0.251802</td>\n",
       "      <td>-0.737261</td>\n",
       "      <td>0.139444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 02:00:00</th>\n",
       "      <td>-1.912116</td>\n",
       "      <td>-1.114317</td>\n",
       "      <td>0.183266</td>\n",
       "      <td>-0.822786</td>\n",
       "      <td>0.770174</td>\n",
       "      <td>-0.876490</td>\n",
       "      <td>-1.013548</td>\n",
       "      <td>0.554085</td>\n",
       "      <td>-0.846222</td>\n",
       "      <td>-0.220012</td>\n",
       "      <td>-0.574831</td>\n",
       "      <td>-0.192518</td>\n",
       "      <td>0.607449</td>\n",
       "      <td>-1.394479</td>\n",
       "      <td>-0.250240</td>\n",
       "      <td>-0.854941</td>\n",
       "      <td>0.139444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 03:00:00</th>\n",
       "      <td>-1.809045</td>\n",
       "      <td>-0.783042</td>\n",
       "      <td>-0.378111</td>\n",
       "      <td>0.341156</td>\n",
       "      <td>0.621194</td>\n",
       "      <td>-0.290211</td>\n",
       "      <td>-0.826469</td>\n",
       "      <td>-0.211185</td>\n",
       "      <td>-0.404306</td>\n",
       "      <td>0.218373</td>\n",
       "      <td>-0.324709</td>\n",
       "      <td>0.325491</td>\n",
       "      <td>0.491460</td>\n",
       "      <td>-0.903670</td>\n",
       "      <td>-0.047015</td>\n",
       "      <td>-0.621963</td>\n",
       "      <td>0.139444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 04:00:00</th>\n",
       "      <td>-1.338802</td>\n",
       "      <td>-0.484910</td>\n",
       "      <td>0.072745</td>\n",
       "      <td>1.023019</td>\n",
       "      <td>0.467629</td>\n",
       "      <td>-0.478080</td>\n",
       "      <td>-0.601238</td>\n",
       "      <td>1.016033</td>\n",
       "      <td>-0.371487</td>\n",
       "      <td>0.097253</td>\n",
       "      <td>-0.313432</td>\n",
       "      <td>0.201600</td>\n",
       "      <td>0.466528</td>\n",
       "      <td>-0.906983</td>\n",
       "      <td>-0.234798</td>\n",
       "      <td>-0.368182</td>\n",
       "      <td>0.139444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         bx_gse_mean  bx_gse_std  by_gse_mean  by_gse_std  \\\n",
       "period  timedelta                                                           \n",
       "train_a 0 days 00:00:00    -1.599207   -0.381502     0.419516    0.031658   \n",
       "        0 days 01:00:00    -1.757995   -0.867747     0.179257   -0.272971   \n",
       "        0 days 02:00:00    -1.912116   -1.114317     0.183266   -0.822786   \n",
       "        0 days 03:00:00    -1.809045   -0.783042    -0.378111    0.341156   \n",
       "        0 days 04:00:00    -1.338802   -0.484910     0.072745    1.023019   \n",
       "\n",
       "                         bz_gse_mean  bz_gse_std  speed_mean  speed_std  \\\n",
       "period  timedelta                                                         \n",
       "train_a 0 days 00:00:00     0.300358   -0.651645   -0.738546   0.862524   \n",
       "        0 days 01:00:00     0.446103   -0.517913   -0.986904   0.995063   \n",
       "        0 days 02:00:00     0.770174   -0.876490   -1.013548   0.554085   \n",
       "        0 days 03:00:00     0.621194   -0.290211   -0.826469  -0.211185   \n",
       "        0 days 04:00:00     0.467629   -0.478080   -0.601238   1.016033   \n",
       "\n",
       "                         density_mean  density_std  temperature_mean  \\\n",
       "period  timedelta                                                      \n",
       "train_a 0 days 00:00:00     -0.775827    -0.205724         -0.375267   \n",
       "        0 days 01:00:00     -0.861692    -0.058215         -0.479430   \n",
       "        0 days 02:00:00     -0.846222    -0.220012         -0.574831   \n",
       "        0 days 03:00:00     -0.404306     0.218373         -0.324709   \n",
       "        0 days 04:00:00     -0.371487     0.097253         -0.313432   \n",
       "\n",
       "                         temperature_std  theta_gse_mean  theta_gse_std  \\\n",
       "period  timedelta                                                         \n",
       "train_a 0 days 00:00:00         0.383941        0.262090      -1.146417   \n",
       "        0 days 01:00:00         0.953178        0.378546      -1.054098   \n",
       "        0 days 02:00:00        -0.192518        0.607449      -1.394479   \n",
       "        0 days 03:00:00         0.325491        0.491460      -0.903670   \n",
       "        0 days 04:00:00         0.201600        0.466528      -0.906983   \n",
       "\n",
       "                         phi_gse_mean  phi_gse_std  smoothed_ssn  \n",
       "period  timedelta                                                 \n",
       "train_a 0 days 00:00:00     -0.364011    -0.705389      0.139444  \n",
       "        0 days 01:00:00     -0.251802    -0.737261      0.139444  \n",
       "        0 days 02:00:00     -0.250240    -0.854941      0.139444  \n",
       "        0 days 03:00:00     -0.047015    -0.621963      0.139444  \n",
       "        0 days 04:00:00     -0.234798    -0.368182      0.139444  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, scaler = preprocess_features(solar_wind, sunspots, subset=SOLAR_WIND_FEATURES)\n",
    "print(features.shape)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RiezmkUvW_60"
   },
   "outputs": [],
   "source": [
    "# check to make sure missing values are filled\n",
    "assert (features.isna().sum() == 0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "fNF-BjxQXCOi",
    "outputId": "c9aaa55a-ae0d-4657-8bd3-f9654ab7640d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>t0</th>\n",
       "      <th>t1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>period</th>\n",
       "      <th>timedelta</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">train_a</th>\n",
       "      <th>0 days 00:00:00</th>\n",
       "      <td>-7</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 01:00:00</th>\n",
       "      <td>-10</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 02:00:00</th>\n",
       "      <td>-10</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 03:00:00</th>\n",
       "      <td>-6</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 04:00:00</th>\n",
       "      <td>-2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         t0    t1\n",
       "period  timedelta                \n",
       "train_a 0 days 00:00:00  -7 -10.0\n",
       "        0 days 01:00:00 -10 -10.0\n",
       "        0 days 02:00:00 -10  -6.0\n",
       "        0 days 03:00:00  -6  -2.0\n",
       "        0 days 04:00:00  -2   3.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YCOLS = [\"t0\", \"t1\"]\n",
    "\n",
    "\n",
    "def process_labels(dst):\n",
    "    y = dst.copy()\n",
    "    y[\"t1\"] = y.groupby(\"period\").dst.shift(-1)\n",
    "    y.columns = YCOLS\n",
    "    return y\n",
    "\n",
    "\n",
    "labels = process_labels(dst)\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "abqBOmG-XEHT",
    "outputId": "bd2ee012-f728-428e-b634-ab64d188627d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>t0</th>\n",
       "      <th>t1</th>\n",
       "      <th>bx_gse_mean</th>\n",
       "      <th>bx_gse_std</th>\n",
       "      <th>by_gse_mean</th>\n",
       "      <th>by_gse_std</th>\n",
       "      <th>bz_gse_mean</th>\n",
       "      <th>bz_gse_std</th>\n",
       "      <th>speed_mean</th>\n",
       "      <th>speed_std</th>\n",
       "      <th>density_mean</th>\n",
       "      <th>density_std</th>\n",
       "      <th>temperature_mean</th>\n",
       "      <th>temperature_std</th>\n",
       "      <th>theta_gse_mean</th>\n",
       "      <th>theta_gse_std</th>\n",
       "      <th>phi_gse_mean</th>\n",
       "      <th>phi_gse_std</th>\n",
       "      <th>smoothed_ssn</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>period</th>\n",
       "      <th>timedelta</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">train_a</th>\n",
       "      <th>0 days 00:00:00</th>\n",
       "      <td>-7</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-1.599207</td>\n",
       "      <td>-0.381502</td>\n",
       "      <td>0.419516</td>\n",
       "      <td>0.031658</td>\n",
       "      <td>0.300358</td>\n",
       "      <td>-0.651645</td>\n",
       "      <td>-0.738546</td>\n",
       "      <td>0.862524</td>\n",
       "      <td>-0.775827</td>\n",
       "      <td>-0.205724</td>\n",
       "      <td>-0.375267</td>\n",
       "      <td>0.383941</td>\n",
       "      <td>0.262090</td>\n",
       "      <td>-1.146417</td>\n",
       "      <td>-0.364011</td>\n",
       "      <td>-0.705389</td>\n",
       "      <td>0.139444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 01:00:00</th>\n",
       "      <td>-10</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-1.757995</td>\n",
       "      <td>-0.867747</td>\n",
       "      <td>0.179257</td>\n",
       "      <td>-0.272971</td>\n",
       "      <td>0.446103</td>\n",
       "      <td>-0.517913</td>\n",
       "      <td>-0.986904</td>\n",
       "      <td>0.995063</td>\n",
       "      <td>-0.861692</td>\n",
       "      <td>-0.058215</td>\n",
       "      <td>-0.479430</td>\n",
       "      <td>0.953178</td>\n",
       "      <td>0.378546</td>\n",
       "      <td>-1.054098</td>\n",
       "      <td>-0.251802</td>\n",
       "      <td>-0.737261</td>\n",
       "      <td>0.139444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 02:00:00</th>\n",
       "      <td>-10</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-1.912116</td>\n",
       "      <td>-1.114317</td>\n",
       "      <td>0.183266</td>\n",
       "      <td>-0.822786</td>\n",
       "      <td>0.770174</td>\n",
       "      <td>-0.876490</td>\n",
       "      <td>-1.013548</td>\n",
       "      <td>0.554085</td>\n",
       "      <td>-0.846222</td>\n",
       "      <td>-0.220012</td>\n",
       "      <td>-0.574831</td>\n",
       "      <td>-0.192518</td>\n",
       "      <td>0.607449</td>\n",
       "      <td>-1.394479</td>\n",
       "      <td>-0.250240</td>\n",
       "      <td>-0.854941</td>\n",
       "      <td>0.139444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 03:00:00</th>\n",
       "      <td>-6</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.809045</td>\n",
       "      <td>-0.783042</td>\n",
       "      <td>-0.378111</td>\n",
       "      <td>0.341156</td>\n",
       "      <td>0.621194</td>\n",
       "      <td>-0.290211</td>\n",
       "      <td>-0.826469</td>\n",
       "      <td>-0.211185</td>\n",
       "      <td>-0.404306</td>\n",
       "      <td>0.218373</td>\n",
       "      <td>-0.324709</td>\n",
       "      <td>0.325491</td>\n",
       "      <td>0.491460</td>\n",
       "      <td>-0.903670</td>\n",
       "      <td>-0.047015</td>\n",
       "      <td>-0.621963</td>\n",
       "      <td>0.139444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 days 04:00:00</th>\n",
       "      <td>-2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.338802</td>\n",
       "      <td>-0.484910</td>\n",
       "      <td>0.072745</td>\n",
       "      <td>1.023019</td>\n",
       "      <td>0.467629</td>\n",
       "      <td>-0.478080</td>\n",
       "      <td>-0.601238</td>\n",
       "      <td>1.016033</td>\n",
       "      <td>-0.371487</td>\n",
       "      <td>0.097253</td>\n",
       "      <td>-0.313432</td>\n",
       "      <td>0.201600</td>\n",
       "      <td>0.466528</td>\n",
       "      <td>-0.906983</td>\n",
       "      <td>-0.234798</td>\n",
       "      <td>-0.368182</td>\n",
       "      <td>0.139444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         t0    t1  bx_gse_mean  bx_gse_std  by_gse_mean  \\\n",
       "period  timedelta                                                         \n",
       "train_a 0 days 00:00:00  -7 -10.0    -1.599207   -0.381502     0.419516   \n",
       "        0 days 01:00:00 -10 -10.0    -1.757995   -0.867747     0.179257   \n",
       "        0 days 02:00:00 -10  -6.0    -1.912116   -1.114317     0.183266   \n",
       "        0 days 03:00:00  -6  -2.0    -1.809045   -0.783042    -0.378111   \n",
       "        0 days 04:00:00  -2   3.0    -1.338802   -0.484910     0.072745   \n",
       "\n",
       "                         by_gse_std  bz_gse_mean  bz_gse_std  speed_mean  \\\n",
       "period  timedelta                                                          \n",
       "train_a 0 days 00:00:00    0.031658     0.300358   -0.651645   -0.738546   \n",
       "        0 days 01:00:00   -0.272971     0.446103   -0.517913   -0.986904   \n",
       "        0 days 02:00:00   -0.822786     0.770174   -0.876490   -1.013548   \n",
       "        0 days 03:00:00    0.341156     0.621194   -0.290211   -0.826469   \n",
       "        0 days 04:00:00    1.023019     0.467629   -0.478080   -0.601238   \n",
       "\n",
       "                         speed_std  density_mean  density_std  \\\n",
       "period  timedelta                                               \n",
       "train_a 0 days 00:00:00   0.862524     -0.775827    -0.205724   \n",
       "        0 days 01:00:00   0.995063     -0.861692    -0.058215   \n",
       "        0 days 02:00:00   0.554085     -0.846222    -0.220012   \n",
       "        0 days 03:00:00  -0.211185     -0.404306     0.218373   \n",
       "        0 days 04:00:00   1.016033     -0.371487     0.097253   \n",
       "\n",
       "                         temperature_mean  temperature_std  theta_gse_mean  \\\n",
       "period  timedelta                                                            \n",
       "train_a 0 days 00:00:00         -0.375267         0.383941        0.262090   \n",
       "        0 days 01:00:00         -0.479430         0.953178        0.378546   \n",
       "        0 days 02:00:00         -0.574831        -0.192518        0.607449   \n",
       "        0 days 03:00:00         -0.324709         0.325491        0.491460   \n",
       "        0 days 04:00:00         -0.313432         0.201600        0.466528   \n",
       "\n",
       "                         theta_gse_std  phi_gse_mean  phi_gse_std  \\\n",
       "period  timedelta                                                   \n",
       "train_a 0 days 00:00:00      -1.146417     -0.364011    -0.705389   \n",
       "        0 days 01:00:00      -1.054098     -0.251802    -0.737261   \n",
       "        0 days 02:00:00      -1.394479     -0.250240    -0.854941   \n",
       "        0 days 03:00:00      -0.903670     -0.047015    -0.621963   \n",
       "        0 days 04:00:00      -0.906983     -0.234798    -0.368182   \n",
       "\n",
       "                         smoothed_ssn  \n",
       "period  timedelta                      \n",
       "train_a 0 days 00:00:00      0.139444  \n",
       "        0 days 01:00:00      0.139444  \n",
       "        0 days 02:00:00      0.139444  \n",
       "        0 days 03:00:00      0.139444  \n",
       "        0 days 04:00:00      0.139444  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = labels.join(features)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IYJu4DPGXJW7"
   },
   "outputs": [],
   "source": [
    "data=data[['t0', 't1','bx_gse_mean', 'by_gse_mean',\n",
    "       'bz_gse_mean', 'bz_gse_std', 'speed_mean', 'speed_std', 'density_mean',\n",
    "        'smoothed_ssn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OM5W5ukOXMSC"
   },
   "outputs": [],
   "source": [
    "def get_train_test_val(data, test_per_period, val_per_period):\n",
    "    \"\"\"Splits data across periods into train, test, and validation\"\"\"\n",
    "    # assign the last `test_per_period` rows from each period to test\n",
    "    test = data.groupby(\"period\").tail(test_per_period)\n",
    "    interim = data[~data.index.isin(test.index)]\n",
    "    # assign the last `val_per_period` from the remaining rows to validation\n",
    "    val = interim.groupby(\"period\").tail(val_per_period)\n",
    "    # the remaining rows are assigned to train\n",
    "    train = interim[~interim.index.isin(val.index)]\n",
    "    return train, test, val\n",
    "\n",
    "\n",
    "train, test, val = get_train_test_val(data, test_per_period=6_000, val_per_period=6_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QByjIL_aXRdK",
    "outputId": "e58f9a99-4d18-4cdb-8e90-04ce27e42276"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['t0', 't1', 'bx_gse_mean', 'by_gse_mean', 'bz_gse_mean', 'bz_gse_std',\n",
       "       'speed_mean', 'speed_std', 'density_mean', 'smoothed_ssn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XCOLS=data.columns\n",
    "XCOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXZRKj6CXUTD",
    "outputId": "69928425-4531-48d1-f701-4c7dbcc3ae6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches: 748\n",
      "Number of val batches: 126\n"
     ]
    }
   ],
   "source": [
    "data_config = {\n",
    "    \"timesteps\": 138,\n",
    "    \"batch_size\":138,\n",
    "}\n",
    "\n",
    "\n",
    "def timeseries_dataset_from_df(df, batch_size):\n",
    "    dataset = None\n",
    "    timesteps = data_config[\"timesteps\"]\n",
    "\n",
    "    # iterate through periods\n",
    "    for _, period_df in df.groupby(\"period\"):\n",
    "        # realign features and labels so that first sequence of 32 is aligned with the 33rd target\n",
    "        inputs = period_df[XCOLS][:-timesteps]\n",
    "        outputs = period_df[YCOLS][timesteps:]\n",
    "\n",
    "        period_ds = preprocessing.timeseries_dataset_from_array(\n",
    "            inputs,\n",
    "            outputs,\n",
    "            timesteps,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        if dataset is None:\n",
    "            dataset = period_ds\n",
    "        else:\n",
    "            dataset = dataset.concatenate(period_ds)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_ds = timeseries_dataset_from_df(train, data_config[\"batch_size\"])\n",
    "val_ds = timeseries_dataset_from_df(val, data_config[\"batch_size\"])\n",
    "\n",
    "print(f\"Number of train batches: {len(train_ds)}\")\n",
    "print(f\"Number of val batches: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HAyqRRGVXXZK",
    "outputId": "4817525a-d791-4227-b698-8f4c2e2f5fed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139872, 10)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oVcdUwsFXZlk",
    "outputId": "9113e032-3ef4-4edd-abc8-10614ceab42b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 32)                5504      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 7,682\n",
      "Trainable params: 7,682\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define our model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "model_config = {\"n_epochs\": 180, \"n_neurons\": 32,\n",
    "                  \"dropout\": 0.4, \"stateful\": False}\n",
    "\n",
    "def models():\n",
    "  \n",
    "  model = Sequential()\n",
    "  model.add(\n",
    "      LSTM(\n",
    "          model_config[\"n_neurons\"],\n",
    "          # usually set to (batch_size, sequence_length, n_features)\n",
    "          # setting the batch size to None allows for variable length batches\n",
    "          batch_input_shape=(None, data_config[\"timesteps\"], len(XCOLS)),\n",
    "          stateful=model_config[\"stateful\"],\n",
    "          kernel_regularizer='l2',\n",
    "      )\n",
    "  )\n",
    " \n",
    "  model.add(Dense(32,kernel_regularizer='l2'))\n",
    "  model.add(Dropout(0.4))\n",
    "  model.add(Dense(32,kernel_regularizer='l2'))\n",
    "  model.add(Dropout(0.4))\n",
    "  #model.add(Dense())\n",
    "  model.add(Dense(len(YCOLS)))\n",
    "  opt=Adam(learning_rate=0.00001)\n",
    "  model.compile(\n",
    "      loss=\"mean_squared_error\",\n",
    "      optimizer=opt,\n",
    "  )\n",
    "  return model\n",
    "\n",
    "model_final4=models()\n",
    "model_final4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GYzIrsoqXb0j",
    "outputId": "6fd8c34f-15cb-41dd-f842-cec6e3159658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "748/748 [==============================] - 39s 50ms/step - loss: 604.2795 - val_loss: 491.7629\n",
      "Epoch 2/180\n",
      "748/748 [==============================] - 39s 52ms/step - loss: 581.2033 - val_loss: 471.5912\n",
      "Epoch 3/180\n",
      "748/748 [==============================] - 38s 51ms/step - loss: 551.1778 - val_loss: 445.0203\n",
      "Epoch 4/180\n",
      "748/748 [==============================] - 37s 50ms/step - loss: 513.7193 - val_loss: 413.3471\n",
      "Epoch 5/180\n",
      "748/748 [==============================] - 37s 50ms/step - loss: 470.9299 - val_loss: 382.0183\n",
      "Epoch 6/180\n",
      "748/748 [==============================] - 38s 51ms/step - loss: 430.7098 - val_loss: 353.4606\n",
      "Epoch 7/180\n",
      "748/748 [==============================] - 37s 49ms/step - loss: 394.3636 - val_loss: 328.3347\n",
      "Epoch 8/180\n",
      "748/748 [==============================] - 39s 52ms/step - loss: 362.2748 - val_loss: 306.1612\n",
      "Epoch 9/180\n",
      "748/748 [==============================] - 39s 52ms/step - loss: 333.4778 - val_loss: 286.5801\n",
      "Epoch 10/180\n",
      "748/748 [==============================] - 38s 51ms/step - loss: 308.3393 - val_loss: 268.9710\n",
      "Epoch 11/180\n",
      "748/748 [==============================] - 38s 51ms/step - loss: 286.2218 - val_loss: 252.9957\n",
      "Epoch 12/180\n",
      "748/748 [==============================] - 37s 50ms/step - loss: 266.3352 - val_loss: 238.4813\n",
      "Epoch 13/180\n",
      "748/748 [==============================] - 42s 56ms/step - loss: 245.8888 - val_loss: 225.1951\n",
      "Epoch 14/180\n",
      "748/748 [==============================] - 50s 67ms/step - loss: 231.1822 - val_loss: 213.3354\n",
      "Epoch 15/180\n",
      "748/748 [==============================] - 44s 58ms/step - loss: 214.5758 - val_loss: 202.6730\n",
      "Epoch 16/180\n",
      "748/748 [==============================] - 43s 58ms/step - loss: 202.4330 - val_loss: 193.0865\n",
      "Epoch 17/180\n",
      "748/748 [==============================] - 41s 55ms/step - loss: 189.5167 - val_loss: 184.7915\n",
      "Epoch 18/180\n",
      "748/748 [==============================] - 40s 54ms/step - loss: 179.8256 - val_loss: 176.8955\n",
      "Epoch 19/180\n",
      "748/748 [==============================] - 41s 55ms/step - loss: 171.3116 - val_loss: 169.9122\n",
      "Epoch 20/180\n",
      "748/748 [==============================] - 41s 54ms/step - loss: 161.4174 - val_loss: 163.4672\n",
      "Epoch 21/180\n",
      "748/748 [==============================] - 37s 50ms/step - loss: 156.5773 - val_loss: 157.4048\n",
      "Epoch 22/180\n",
      "748/748 [==============================] - 43s 57ms/step - loss: 147.0693 - val_loss: 151.8334\n",
      "Epoch 23/180\n",
      "748/748 [==============================] - 36s 48ms/step - loss: 141.3059 - val_loss: 146.7141\n",
      "Epoch 24/180\n",
      "748/748 [==============================] - 42s 56ms/step - loss: 135.7623 - val_loss: 141.7757\n",
      "Epoch 25/180\n",
      "748/748 [==============================] - 38s 51ms/step - loss: 130.3786 - val_loss: 137.4125\n",
      "Epoch 26/180\n",
      "748/748 [==============================] - 42s 56ms/step - loss: 126.6621 - val_loss: 132.9545\n",
      "Epoch 27/180\n",
      "748/748 [==============================] - 36s 48ms/step - loss: 122.2409 - val_loss: 129.0086\n",
      "Epoch 28/180\n",
      "748/748 [==============================] - 39s 52ms/step - loss: 117.8557 - val_loss: 125.2390\n",
      "Epoch 29/180\n",
      "748/748 [==============================] - 37s 50ms/step - loss: 114.5873 - val_loss: 121.7981\n",
      "Epoch 30/180\n",
      "748/748 [==============================] - 37s 50ms/step - loss: 110.9291 - val_loss: 118.4536\n",
      "Epoch 31/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 107.8559 - val_loss: 115.3263\n",
      "Epoch 32/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 103.2497 - val_loss: 112.2504\n",
      "Epoch 33/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 100.7864 - val_loss: 109.4357\n",
      "Epoch 34/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 97.4597 - val_loss: 106.5489\n",
      "Epoch 35/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 96.7325 - val_loss: 103.9541\n",
      "Epoch 36/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 93.4556 - val_loss: 101.5401\n",
      "Epoch 37/180\n",
      "748/748 [==============================] - 36s 48ms/step - loss: 90.8758 - val_loss: 99.2698\n",
      "Epoch 38/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 89.3142 - val_loss: 96.8733\n",
      "Epoch 39/180\n",
      "748/748 [==============================] - 38s 51ms/step - loss: 84.7880 - val_loss: 94.8247\n",
      "Epoch 40/180\n",
      "748/748 [==============================] - 38s 51ms/step - loss: 83.9595 - val_loss: 92.7458\n",
      "Epoch 41/180\n",
      "748/748 [==============================] - 36s 48ms/step - loss: 83.5527 - val_loss: 90.7276\n",
      "Epoch 42/180\n",
      "748/748 [==============================] - 36s 48ms/step - loss: 80.1239 - val_loss: 88.8829\n",
      "Epoch 43/180\n",
      "748/748 [==============================] - 36s 48ms/step - loss: 78.0651 - val_loss: 87.0387\n",
      "Epoch 44/180\n",
      "748/748 [==============================] - 36s 48ms/step - loss: 79.4166 - val_loss: 85.4277\n",
      "Epoch 45/180\n",
      "748/748 [==============================] - 36s 48ms/step - loss: 77.4810 - val_loss: 83.5316\n",
      "Epoch 46/180\n",
      "748/748 [==============================] - 36s 48ms/step - loss: 74.3086 - val_loss: 82.0132\n",
      "Epoch 47/180\n",
      "748/748 [==============================] - 36s 48ms/step - loss: 73.8426 - val_loss: 80.3120\n",
      "Epoch 48/180\n",
      "748/748 [==============================] - 37s 49ms/step - loss: 70.7155 - val_loss: 78.8407\n",
      "Epoch 49/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 72.6266 - val_loss: 77.5331\n",
      "Epoch 50/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 70.7990 - val_loss: 76.1576\n",
      "Epoch 51/180\n",
      "748/748 [==============================] - 41s 54ms/step - loss: 68.5419 - val_loss: 74.7023\n",
      "Epoch 52/180\n",
      "748/748 [==============================] - 37s 50ms/step - loss: 67.5323 - val_loss: 73.2661\n",
      "Epoch 53/180\n",
      "748/748 [==============================] - 37s 50ms/step - loss: 66.2678 - val_loss: 71.9605\n",
      "Epoch 54/180\n",
      "748/748 [==============================] - 36s 48ms/step - loss: 66.6632 - val_loss: 70.6896\n",
      "Epoch 55/180\n",
      "748/748 [==============================] - 38s 51ms/step - loss: 66.5200 - val_loss: 69.4517\n",
      "Epoch 56/180\n",
      "748/748 [==============================] - 40s 53ms/step - loss: 64.5026 - val_loss: 68.1053\n",
      "Epoch 57/180\n",
      "748/748 [==============================] - 38s 50ms/step - loss: 62.7853 - val_loss: 66.9889\n",
      "Epoch 58/180\n",
      "748/748 [==============================] - 39s 52ms/step - loss: 61.6354 - val_loss: 65.9367\n",
      "Epoch 59/180\n",
      "748/748 [==============================] - 37s 49ms/step - loss: 61.1842 - val_loss: 64.6284\n",
      "Epoch 60/180\n",
      "748/748 [==============================] - 39s 52ms/step - loss: 60.0498 - val_loss: 63.5301\n",
      "Epoch 61/180\n",
      "748/748 [==============================] - 38s 50ms/step - loss: 59.1935 - val_loss: 62.5979\n",
      "Epoch 62/180\n",
      "748/748 [==============================] - 42s 56ms/step - loss: 60.8984 - val_loss: 61.6983\n",
      "Epoch 63/180\n",
      "748/748 [==============================] - 38s 50ms/step - loss: 58.9633 - val_loss: 60.5411\n",
      "Epoch 64/180\n",
      "748/748 [==============================] - 39s 52ms/step - loss: 59.6959 - val_loss: 59.7133\n",
      "Epoch 65/180\n",
      "748/748 [==============================] - 41s 55ms/step - loss: 56.8360 - val_loss: 58.6990\n",
      "Epoch 66/180\n",
      "748/748 [==============================] - 38s 50ms/step - loss: 56.9482 - val_loss: 57.6761\n",
      "Epoch 67/180\n",
      "748/748 [==============================] - 39s 52ms/step - loss: 55.6422 - val_loss: 56.8837\n",
      "Epoch 68/180\n",
      "748/748 [==============================] - 38s 51ms/step - loss: 55.2288 - val_loss: 56.2985\n",
      "Epoch 69/180\n",
      "748/748 [==============================] - 39s 52ms/step - loss: 55.1973 - val_loss: 55.3712\n",
      "Epoch 70/180\n",
      "748/748 [==============================] - 37s 49ms/step - loss: 56.0475 - val_loss: 54.7243\n",
      "Epoch 71/180\n",
      "748/748 [==============================] - 37s 49ms/step - loss: 56.7340 - val_loss: 53.6272\n",
      "Epoch 72/180\n",
      "748/748 [==============================] - 37s 50ms/step - loss: 54.9510 - val_loss: 52.7941\n",
      "Epoch 73/180\n",
      "748/748 [==============================] - 37s 49ms/step - loss: 54.8306 - val_loss: 52.0379\n",
      "Epoch 74/180\n",
      "748/748 [==============================] - 36s 48ms/step - loss: 52.8590 - val_loss: 51.5762\n",
      "Epoch 75/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 53.5687 - val_loss: 50.5510\n",
      "Epoch 76/180\n",
      "748/748 [==============================] - 35s 46ms/step - loss: 52.6187 - val_loss: 50.0740\n",
      "Epoch 77/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748/748 [==============================] - 34s 46ms/step - loss: 52.2339 - val_loss: 49.3125\n",
      "Epoch 78/180\n",
      "748/748 [==============================] - 34s 45ms/step - loss: 52.4107 - val_loss: 48.5247\n",
      "Epoch 79/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 51.2727 - val_loss: 47.8947\n",
      "Epoch 80/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 50.5184 - val_loss: 47.1614\n",
      "Epoch 81/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 50.2589 - val_loss: 46.7547\n",
      "Epoch 82/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 51.5306 - val_loss: 46.1984\n",
      "Epoch 83/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 49.5366 - val_loss: 45.4514\n",
      "Epoch 84/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 48.2533 - val_loss: 45.0358\n",
      "Epoch 85/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 49.7660 - val_loss: 44.1887\n",
      "Epoch 86/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 47.7125 - val_loss: 43.4761\n",
      "Epoch 87/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 48.3679 - val_loss: 43.0158\n",
      "Epoch 88/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 48.0377 - val_loss: 42.5376\n",
      "Epoch 89/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 47.3114 - val_loss: 42.0085\n",
      "Epoch 90/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 48.6666 - val_loss: 41.7208\n",
      "Epoch 91/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 46.4339 - val_loss: 41.1341\n",
      "Epoch 92/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 45.8183 - val_loss: 40.5384\n",
      "Epoch 93/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 46.8285 - val_loss: 40.1386\n",
      "Epoch 94/180\n",
      "748/748 [==============================] - 34s 45ms/step - loss: 46.0440 - val_loss: 39.8167\n",
      "Epoch 95/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 44.9003 - val_loss: 39.1519\n",
      "Epoch 96/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 47.3770 - val_loss: 38.9298\n",
      "Epoch 97/180\n",
      "748/748 [==============================] - 35s 46ms/step - loss: 46.4279 - val_loss: 38.2034\n",
      "Epoch 98/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 46.7128 - val_loss: 38.3022\n",
      "Epoch 99/180\n",
      "748/748 [==============================] - 35s 46ms/step - loss: 45.7044 - val_loss: 37.7652\n",
      "Epoch 100/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 45.5130 - val_loss: 37.5988\n",
      "Epoch 101/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 44.8707 - val_loss: 37.2589\n",
      "Epoch 102/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 46.8666 - val_loss: 36.5057\n",
      "Epoch 103/180\n",
      "748/748 [==============================] - 35s 46ms/step - loss: 44.0470 - val_loss: 36.1140\n",
      "Epoch 104/180\n",
      "748/748 [==============================] - 35s 46ms/step - loss: 45.7370 - val_loss: 35.5709\n",
      "Epoch 105/180\n",
      "748/748 [==============================] - 35s 46ms/step - loss: 44.9385 - val_loss: 35.2126\n",
      "Epoch 106/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 44.5454 - val_loss: 35.1262\n",
      "Epoch 107/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 43.9988 - val_loss: 34.7100\n",
      "Epoch 108/180\n",
      "748/748 [==============================] - 35s 46ms/step - loss: 44.2488 - val_loss: 34.2789\n",
      "Epoch 109/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 44.2102 - val_loss: 33.8517\n",
      "Epoch 110/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 43.1192 - val_loss: 33.8400\n",
      "Epoch 111/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 42.4897 - val_loss: 33.4552\n",
      "Epoch 112/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 43.0462 - val_loss: 33.0343\n",
      "Epoch 113/180\n",
      "748/748 [==============================] - 35s 46ms/step - loss: 43.6881 - val_loss: 33.0392\n",
      "Epoch 114/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 43.4865 - val_loss: 32.4854\n",
      "Epoch 115/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 43.1400 - val_loss: 32.1097\n",
      "Epoch 116/180\n",
      "748/748 [==============================] - 35s 46ms/step - loss: 42.2081 - val_loss: 32.0352\n",
      "Epoch 117/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 42.6422 - val_loss: 31.7733\n",
      "Epoch 118/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 43.2219 - val_loss: 31.4673\n",
      "Epoch 119/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 41.4148 - val_loss: 31.3812\n",
      "Epoch 120/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 41.4519 - val_loss: 31.1162\n",
      "Epoch 121/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 40.8325 - val_loss: 31.4476\n",
      "Epoch 122/180\n",
      "748/748 [==============================] - 35s 46ms/step - loss: 41.8485 - val_loss: 30.6242\n",
      "Epoch 123/180\n",
      "748/748 [==============================] - 35s 46ms/step - loss: 41.4157 - val_loss: 30.6624\n",
      "Epoch 124/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 43.0975 - val_loss: 30.5101\n",
      "Epoch 125/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 40.6494 - val_loss: 30.0349\n",
      "Epoch 126/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 41.5128 - val_loss: 29.9085\n",
      "Epoch 127/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 42.0179 - val_loss: 29.3830\n",
      "Epoch 128/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 39.7363 - val_loss: 29.5750\n",
      "Epoch 129/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 41.0751 - val_loss: 28.8723\n",
      "Epoch 130/180\n",
      "748/748 [==============================] - 35s 47ms/step - loss: 40.6961 - val_loss: 28.7176\n",
      "Epoch 131/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 39.9983 - val_loss: 28.2707\n",
      "Epoch 132/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 41.5040 - val_loss: 28.5074\n",
      "Epoch 133/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 40.1204 - val_loss: 28.2274\n",
      "Epoch 134/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 39.2316 - val_loss: 27.8337\n",
      "Epoch 135/180\n",
      "748/748 [==============================] - 35s 46ms/step - loss: 40.4854 - val_loss: 27.7205\n",
      "Epoch 136/180\n",
      "748/748 [==============================] - 34s 45ms/step - loss: 40.2537 - val_loss: 27.5645\n",
      "Epoch 137/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 39.8116 - val_loss: 27.5414\n",
      "Epoch 138/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 38.7068 - val_loss: 27.5704\n",
      "Epoch 139/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 39.3382 - val_loss: 27.0652\n",
      "Epoch 140/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 41.0706 - val_loss: 26.8004\n",
      "Epoch 141/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 40.7949 - val_loss: 26.7790\n",
      "Epoch 142/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 39.2739 - val_loss: 26.6936\n",
      "Epoch 143/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 39.7149 - val_loss: 26.7682\n",
      "Epoch 144/180\n",
      "748/748 [==============================] - 35s 46ms/step - loss: 39.6030 - val_loss: 26.5911\n",
      "Epoch 145/180\n",
      "748/748 [==============================] - 35s 46ms/step - loss: 39.6120 - val_loss: 26.7284\n",
      "Epoch 146/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 39.8580 - val_loss: 26.4594\n",
      "Epoch 147/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 38.6182 - val_loss: 26.1836\n",
      "Epoch 148/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 40.2968 - val_loss: 25.8665\n",
      "Epoch 149/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 39.7107 - val_loss: 25.9561\n",
      "Epoch 150/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 38.8504 - val_loss: 25.6041\n",
      "Epoch 151/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 38.9471 - val_loss: 25.5395\n",
      "Epoch 152/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 39.0559 - val_loss: 25.2282\n",
      "Epoch 153/180\n",
      "748/748 [==============================] - 35s 46ms/step - loss: 39.1035 - val_loss: 25.2757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 38.8989 - val_loss: 24.9778\n",
      "Epoch 155/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 39.0686 - val_loss: 25.1179\n",
      "Epoch 156/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 38.4990 - val_loss: 25.1928\n",
      "Epoch 157/180\n",
      "748/748 [==============================] - 34s 45ms/step - loss: 38.7247 - val_loss: 24.4950\n",
      "Epoch 158/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 38.2387 - val_loss: 24.2748\n",
      "Epoch 159/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 38.8023 - val_loss: 24.0441\n",
      "Epoch 160/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 38.2305 - val_loss: 24.5714\n",
      "Epoch 161/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 38.3006 - val_loss: 24.0931\n",
      "Epoch 162/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 37.1058 - val_loss: 24.1905\n",
      "Epoch 163/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 39.0234 - val_loss: 24.0929\n",
      "Epoch 164/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 37.2278 - val_loss: 23.5496\n",
      "Epoch 165/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 36.8239 - val_loss: 23.6589\n",
      "Epoch 166/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 38.0233 - val_loss: 23.5699\n",
      "Epoch 167/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 37.4399 - val_loss: 23.7926\n",
      "Epoch 168/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 39.0324 - val_loss: 23.2161\n",
      "Epoch 169/180\n",
      "748/748 [==============================] - 35s 46ms/step - loss: 37.8885 - val_loss: 22.9649\n",
      "Epoch 170/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 38.3503 - val_loss: 23.0203\n",
      "Epoch 171/180\n",
      "748/748 [==============================] - 35s 46ms/step - loss: 37.9697 - val_loss: 22.9427\n",
      "Epoch 172/180\n",
      "748/748 [==============================] - 34s 45ms/step - loss: 37.9819 - val_loss: 23.0439\n",
      "Epoch 173/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 36.5818 - val_loss: 23.0140\n",
      "Epoch 174/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 37.7328 - val_loss: 22.5015\n",
      "Epoch 175/180\n",
      "748/748 [==============================] - 34s 45ms/step - loss: 37.2093 - val_loss: 22.4430\n",
      "Epoch 176/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 37.0597 - val_loss: 22.0574\n",
      "Epoch 177/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 37.8798 - val_loss: 22.1389\n",
      "Epoch 178/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 37.5796 - val_loss: 22.1555\n",
      "Epoch 179/180\n",
      "748/748 [==============================] - 34s 46ms/step - loss: 37.7712 - val_loss: 22.0835\n",
      "Epoch 180/180\n",
      "748/748 [==============================] - 35s 46ms/step - loss: 37.3099 - val_loss: 22.4907\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
    "history_final4 = model_final4.fit(\n",
    "    train_ds,\n",
    "    batch_size=data_config[\"batch_size\"],\n",
    "    epochs=model_config[\"n_epochs\"],\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "modIygY6Xgal",
    "outputId": "6de1b65c-0494-4d56-f10b-f9191929d47b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 3s 19ms/step - loss: 26.7704\n",
      "Test RMSE: 5.1740\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_ds = timeseries_dataset_from_df(test, data_config[\"batch_size\"])\n",
    "mse = model_final4.evaluate(test_ds)\n",
    "print(f\"Test RMSE: {mse**.5:.4f}\")\n",
    "for name, values in history_final4.history.items():\n",
    "    plt.plot(values)\n",
    "    plt.grid(\"True\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748/748 [==============================] - 14s 19ms/step - loss: 11.8614\n",
      "Train RMSE: 3.4440\n"
     ]
    }
   ],
   "source": [
    "train_ds = timeseries_dataset_from_df(train, data_config[\"batch_size\"])\n",
    "tmse = model_final4.evaluate(train_ds)\n",
    "print(f\"Train RMSE: {tmse**.5:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Jan30-2021-moreBatch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
